{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383e247e-5b67-4df9-a315-54c8a84b0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configs:\n",
    "    # Env\n",
    "    env_id=\"NNWorld01-v01\", \n",
    "     - once in the goal state, each action ends the episode and returns a reward of 5\n",
    "     - no clear episodes, continuously train the agent with num_train_steps, \n",
    "       if the agent reaches terminal, just reset environment and keep training \n",
    "     - try to immitate the bandit experiments\n",
    "    # Params\n",
    "    eps_sched_fn=poly(0.5), lr_sched_fn=poly(0.8)\n",
    "    # Algos\n",
    "    haver2, action_sigma=adaptive(1), haver_delta=0.01, haver_const=varied\n",
    "Status:\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_palette(\"tab20\")\n",
    "colors = sns.color_palette(\"bright\")\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "import multiprocessing\n",
    "\n",
    "# import gymnasium as gym\n",
    "import gym\n",
    "import gym_examples\n",
    "from gym.wrappers import FlattenObservation\n",
    "\n",
    "from algos import *\n",
    "from bandit_problem import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470972cc-8018-41a5-a4f2-dd2b76cec119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_mus = [ 0.  0.  0.  0.  0.  0.  0.  0. -5. -5. -5. -5. -5. -5. -5. -5.]\n",
      "action_sigmas = [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
      "optimal_num_steps = 4\n",
      "optimal_reward_per_step = -2.5\n",
      "optimal_vstar = -8.573749999999999\n",
      "\n",
      "-> i_step = 0\n",
      "cur_state = [0 0]\n",
      "action = 5\n",
      "reward = 2.37\n",
      "new_state = [1 5]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-inf -inf -inf -inf -inf 2.37 -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5]\n",
      "Bset_muhats = [2.37]\n",
      "Bset_nvisits = [1.]\n",
      "Q_est = 2.37\n",
      "weightedms_estimator\n",
      "idxes = [5]\n",
      "probs = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Q_est = 2.37\n",
      "\n",
      "-> i_step = 1\n",
      "cur_state = [1 5]\n",
      "action = 5\n",
      "reward = 2.77\n",
      "new_state = [2 5]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-inf -inf -inf -inf -inf 2.37 -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5]\n",
      "Bset_muhats = [2.37]\n",
      "Bset_nvisits = [1.]\n",
      "Q_est = 2.37\n",
      "weightedms_estimator\n",
      "idxes = [5]\n",
      "probs = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Q_est = 2.37\n",
      "\n",
      "-> i_step = 2\n",
      "cur_state = [2 5]\n",
      "action = 6\n",
      "reward = -6.72\n",
      "new_state = [3 6]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-inf -inf -inf -inf -inf 2.37 -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5]\n",
      "Bset_muhats = [2.37]\n",
      "Bset_nvisits = [1.]\n",
      "Q_est = 2.37\n",
      "weightedms_estimator\n",
      "idxes = [5]\n",
      "probs = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Q_est = 2.37\n",
      "\n",
      "-> i_step = 3\n",
      "cur_state = [3 6]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-inf -inf -inf -inf -inf 2.37 -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5]\n",
      "Bset_muhats = [2.37]\n",
      "Bset_nvisits = [1.]\n",
      "Q_est = 2.37\n",
      "weightedms_estimator\n",
      "idxes = [5]\n",
      "probs = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Q_est = 2.37\n",
      "\n",
      "-> i_step = 4\n",
      "cur_state = [0 0]\n",
      "action = 10\n",
      "reward = -1.81\n",
      "new_state = [ 1 10]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  -inf  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5, 10]\n",
      "Bset_muhats = [ 2.37 -1.81]\n",
      "Bset_nvisits = [1. 1.]\n",
      "Q_est = 0.28\n",
      "weightedms_estimator\n",
      "idxes = [ 5 10]\n",
      "probs = [0.   0.   0.   0.   0.   0.51 0.   0.   0.   0.   0.49 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = 0.32\n",
      "\n",
      "-> i_step = 5\n",
      "cur_state = [ 1 10]\n",
      "action = 15\n",
      "reward = -19.13\n",
      "new_state = [ 2 15]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  -inf  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5, 10]\n",
      "Bset_muhats = [ 2.37 -1.81]\n",
      "Bset_nvisits = [1. 1.]\n",
      "Q_est = 0.28\n",
      "weightedms_estimator\n",
      "-> est_name = haver3_1.0\n",
      "haver_const = 1.0\n",
      "\n",
      "idxes = [ 5 10]\n",
      "probs = [0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.5 0.  0.  0.  0.  0. ]\n",
      "Q_est = 0.29\n",
      "\n",
      "-> i_step = 6\n",
      "cur_state = [ 2 15]\n",
      "action = 14\n",
      "reward = 4.36\n",
      "new_state = [ 3 14]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  -inf  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5, 10]\n",
      "Bset_muhats = [ 2.37 -1.81]\n",
      "Bset_nvisits = [1. 1.]\n",
      "Q_est = 0.28\n",
      "weightedms_estimator\n",
      "idxes = [ 5 10]\n",
      "probs = [0.   0.   0.   0.   0.   0.47 0.   0.   0.   0.   0.53 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = 0.18\n",
      "\n",
      "-> i_step = 7\n",
      "cur_state = [ 3 14]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  -inf  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 5\n",
      "action_maxlcb_muhat = 2.37\n",
      "Bset_idxes = [5, 10]\n",
      "Bset_muhats = [ 2.37 -1.81]\n",
      "Bset_nvisits = [1. 1.]\n",
      "Q_est = 0.28\n",
      "weightedms_estimator\n",
      "idxes = [ 5 10]\n",
      "probs = [0.   0.   0.   0.   0.   0.47 0.   0.   0.   0.   0.53 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = 0.18\n",
      "\n",
      "-> i_step = 8\n",
      "cur_state = [0 0]\n",
      "action = 1\n",
      "reward = 3.12\n",
      "new_state = [1 1]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  3.12  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 10]\n",
      "Bset_muhats = [ 3.12  2.37 -1.81]\n",
      "Bset_nvisits = [1. 1. 1.]\n",
      "Q_est = 1.23\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5 10]\n",
      "probs = [0.   0.35 0.   0.   0.   0.34 0.   0.   0.   0.   0.32 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = 1.31\n",
      "\n",
      "-> i_step = 9\n",
      "cur_state = [1 1]\n",
      "action = 3\n",
      "reward = -5.02\n",
      "new_state = [2 3]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  3.12  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 10]\n",
      "Bset_muhats = [ 3.12  2.37 -1.81]\n",
      "Bset_nvisits = [1. 1. 1.]\n",
      "Q_est = 1.23\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5 10]\n",
      "probs = [0.   0.34 0.   0.   0.   0.3  0.   0.   0.   0.   0.36 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = 1.13\n",
      "\n",
      "-> i_step = 10\n",
      "cur_state = [2 3]\n",
      "action = 0\n",
      "reward = -11.01\n",
      "new_state = [3 0]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  3.12  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 10]\n",
      "Bset_muhats = [ 3.12  2.37 -1.81]\n",
      "Bset_nvisits = [1. 1. 1.]\n",
      "Q_est = 1.23\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5 10]\n",
      "probs = [0.   0.32 0.   0.   0.   0.33 0.   0.   0.   0.   0.35 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = 1.15\n",
      "\n",
      "-> i_step = 11\n",
      "cur_state = [3 0]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ -inf  3.12  -inf  -inf  -inf  2.37  -inf  -inf  -inf  -inf -1.81  -inf\n",
      "  -inf  -inf  -inf  -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 10]\n",
      "Bset_muhats = [ 3.12  2.37 -1.81]\n",
      "Bset_nvisits = [1. 1. 1.]\n",
      "Q_est = 1.23\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5 10]\n",
      "probs = [0.   0.34 0.   0.   0.   0.33 0.   0.   0.   0.   0.33 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = 1.26\n",
      "\n",
      "-> i_step = 12\n",
      "cur_state = [0 0]\n",
      "action = 8\n",
      "reward = -15.38\n",
      "new_state = [1 8]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38   -inf\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1.]\n",
      "Q_est = -2.93\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8 10]\n",
      "probs = [0.   0.24 0.   0.   0.   0.27 0.   0.   0.26 0.   0.23 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -3.07\n",
      "\n",
      "-> i_step = 13\n",
      "cur_state = [1 8]\n",
      "action = 12\n",
      "reward = -19.67\n",
      "new_state = [ 2 12]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38   -inf\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1.]\n",
      "Q_est = -2.93\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8 10]\n",
      "probs = [0.   0.26 0.   0.   0.   0.24 0.   0.   0.26 0.   0.24 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -3.11\n",
      "\n",
      "-> i_step = 14\n",
      "cur_state = [ 2 12]\n",
      "action = 10\n",
      "reward = -4.04\n",
      "new_state = [ 3 10]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38   -inf\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1.]\n",
      "Q_est = -2.93\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8 10]\n",
      "probs = [0.   0.26 0.   0.   0.   0.26 0.   0.   0.22 0.   0.26 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -2.45\n",
      "\n",
      "-> i_step = 15\n",
      "cur_state = [ 3 10]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38   -inf\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1.]\n",
      "Q_est = -2.93\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8 10]\n",
      "probs = [0.   0.25 0.   0.   0.   0.23 0.   0.   0.24 0.   0.28 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -2.95\n",
      "\n",
      "-> i_step = 16\n",
      "cur_state = [0 0]\n",
      "action = 9\n",
      "reward = -10.12\n",
      "new_state = [1 9]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1.]\n",
      "Q_est = -4.37\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8  9 10]\n",
      "probs = [0.   0.2  0.   0.   0.   0.22 0.   0.   0.18 0.2  0.2  0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -4.10\n",
      "\n",
      "-> i_step = 17\n",
      "cur_state = [1 9]\n",
      "action = 14\n",
      "reward = -6.34\n",
      "new_state = [ 2 14]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1.]\n",
      "Q_est = -4.37\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8  9 10]\n",
      "probs = [0.   0.21 0.   0.   0.   0.2  0.   0.   0.19 0.2  0.19 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -4.20\n",
      "\n",
      "-> i_step = 18\n",
      "cur_state = [ 2 14]\n",
      "action = 10\n",
      "reward = -1.85\n",
      "new_state = [ 3 10]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1.]\n",
      "Q_est = -4.37\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8  9 10]\n",
      "probs = [0.   0.21 0.   0.   0.   0.18 0.   0.   0.21 0.19 0.2  0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -4.47\n",
      "\n",
      "-> i_step = 19\n",
      "cur_state = [ 3 10]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf   -inf   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1.]\n",
      "Q_est = -4.37\n",
      "weightedms_estimator\n",
      "idxes = [ 1  5  8  9 10]\n",
      "probs = [0.   0.22 0.   0.   0.   0.2  0.   0.   0.19 0.18 0.21 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -4.02\n",
      "\n",
      "-> i_step = 20\n",
      "cur_state = [0 0]\n",
      "action = 4\n",
      "reward = -0.62\n",
      "new_state = [1 4]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.74\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10]\n",
      "probs = [0.   0.17 0.   0.   0.17 0.17 0.   0.   0.17 0.14 0.17 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -3.45\n",
      "\n",
      "-> i_step = 21\n",
      "cur_state = [1 4]\n",
      "action = 3\n",
      "reward = -1.24\n",
      "new_state = [2 3]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.74\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10]\n",
      "probs = [0.   0.18 0.   0.   0.16 0.18 0.   0.   0.15 0.17 0.16 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -3.41\n",
      "\n",
      "-> i_step = 22\n",
      "cur_state = [2 3]\n",
      "action = 6\n",
      "reward = -2.08\n",
      "new_state = [3 6]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.74\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10]\n",
      "probs = [0.   0.14 0.   0.   0.16 0.17 0.   0.   0.16 0.18 0.18 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -3.93\n",
      "\n",
      "-> i_step = 23\n",
      "cur_state = [3 6]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf   -inf   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.74\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10]\n",
      "probs = [0.   0.15 0.   0.   0.17 0.18 0.   0.   0.18 0.16 0.16 0.   0.   0.\n",
      " 0.   0.  ]\n",
      "Q_est = -3.89\n",
      "\n",
      "-> i_step = 24\n",
      "cur_state = [0 0]\n",
      "action = 13\n",
      "reward = -8.08\n",
      "new_state = [ 1 13]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.36\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10 13]\n",
      "probs = [0.   0.15 0.   0.   0.16 0.15 0.   0.   0.12 0.14 0.14 0.   0.   0.14\n",
      " 0.   0.  ]\n",
      "Q_est = -3.84\n",
      "\n",
      "-> i_step = 25\n",
      "cur_state = [ 1 13]\n",
      "action = 13\n",
      "reward = 6.92\n",
      "new_state = [ 2 13]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.36\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10 13]\n",
      "probs = [0.   0.12 0.   0.   0.16 0.15 0.   0.   0.15 0.15 0.13 0.   0.   0.14\n",
      " 0.   0.  ]\n",
      "Q_est = -4.55\n",
      "\n",
      "-> i_step = 26\n",
      "cur_state = [ 2 13]\n",
      "action = 12\n",
      "reward = -5.60\n",
      "new_state = [ 3 12]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.36\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10 13]\n",
      "probs = [0.   0.14 0.   0.   0.14 0.14 0.   0.   0.13 0.14 0.15 0.   0.   0.15\n",
      " 0.   0.  ]\n",
      "Q_est = -4.27\n",
      "\n",
      "-> i_step = 27\n",
      "cur_state = [ 3 12]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf   -inf  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.36\n",
      "weightedms_estimator\n",
      "idxes = [ 1  4  5  8  9 10 13]\n",
      "probs = [0.   0.14 0.   0.   0.15 0.14 0.   0.   0.14 0.15 0.15 0.   0.   0.13\n",
      " 0.   0.  ]\n",
      "Q_est = -4.30\n",
      "\n",
      "-> i_step = 28\n",
      "cur_state = [0 0]\n",
      "action = 3\n",
      "reward = -0.27\n",
      "new_state = [1 3]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13]\n",
      "probs = [0.   0.12 0.   0.13 0.13 0.11 0.   0.   0.14 0.13 0.12 0.   0.   0.11\n",
      " 0.   0.  ]\n",
      "Q_est = -4.12\n",
      "\n",
      "-> i_step = 29\n",
      "cur_state = [1 3]\n",
      "action = 6\n",
      "reward = 9.00\n",
      "new_state = [2 6]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13]\n",
      "probs = [0.   0.12 0.   0.14 0.12 0.12 0.   0.   0.13 0.11 0.13 0.   0.   0.12\n",
      " 0.   0.  ]\n",
      "Q_est = -3.84\n",
      "\n",
      "-> i_step = 30\n",
      "cur_state = [2 6]\n",
      "action = 5\n",
      "reward = 6.50\n",
      "new_state = [3 5]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13]\n",
      "probs = [0.   0.15 0.   0.12 0.13 0.1  0.   0.   0.12 0.12 0.13 0.   0.   0.13\n",
      " 0.   0.  ]\n",
      "Q_est = -3.78\n",
      "\n",
      "-> i_step = 31\n",
      "cur_state = [3 5]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf   -inf]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13]\n",
      "probs = [0.   0.13 0.   0.13 0.15 0.12 0.   0.   0.1  0.12 0.14 0.   0.   0.11\n",
      " 0.   0.  ]\n",
      "Q_est = -3.42\n",
      "\n",
      "-> i_step = 32\n",
      "cur_state = [0 0]\n",
      "action = 15\n",
      "reward = -21.96\n",
      "new_state = [ 1 15]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -5.86\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 15]\n",
      "probs = [0.   0.12 0.   0.13 0.12 0.09 0.   0.   0.1  0.11 0.12 0.   0.   0.12\n",
      " 0.   0.11]\n",
      "Q_est = -5.62\n",
      "\n",
      "-> i_step = 33\n",
      "cur_state = [ 1 15]\n",
      "action = 12\n",
      "reward = -7.45\n",
      "new_state = [ 2 12]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -5.86\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 15]\n",
      "probs = [0.   0.11 0.   0.12 0.11 0.11 0.   0.   0.12 0.12 0.1  0.   0.   0.1\n",
      " 0.   0.11]\n",
      "Q_est = -6.08\n",
      "\n",
      "-> i_step = 34\n",
      "cur_state = [ 2 12]\n",
      "action = 12\n",
      "reward = -7.63\n",
      "new_state = [ 3 12]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -5.86\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 15]\n",
      "probs = [0.   0.12 0.   0.11 0.12 0.1  0.   0.   0.11 0.1  0.11 0.   0.   0.12\n",
      " 0.   0.11]\n",
      "Q_est = -5.84\n",
      "\n",
      "-> i_step = 35\n",
      "cur_state = [ 3 12]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08   -inf -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -5.86\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 15]\n",
      "probs = [0.   0.12 0.   0.13 0.12 0.09 0.   0.   0.11 0.11 0.1  0.   0.   0.1\n",
      " 0.   0.12]\n",
      "Q_est = -5.86\n",
      "\n",
      "-> i_step = 36\n",
      "cur_state = [0 0]\n",
      "action = 14\n",
      "reward = -9.72\n",
      "new_state = [ 1 14]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -6.25\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.09 0.   0.09 0.11 0.1  0.   0.   0.11 0.11 0.1  0.   0.   0.1\n",
      " 0.1  0.1 ]\n",
      "Q_est = -6.43\n",
      "\n",
      "-> i_step = 37\n",
      "cur_state = [ 1 14]\n",
      "action = 14\n",
      "reward = -9.16\n",
      "new_state = [ 2 14]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -6.25\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.11 0.   0.1  0.1  0.08 0.   0.   0.1  0.1  0.1  0.   0.   0.1\n",
      " 0.11 0.09]\n",
      "Q_est = -6.16\n",
      "\n",
      "-> i_step = 38\n",
      "cur_state = [ 2 14]\n",
      "action = 14\n",
      "reward = -3.98\n",
      "new_state = [ 3 14]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -6.25\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.12 0.   0.09 0.09 0.11 0.   0.   0.11 0.1  0.09 0.   0.   0.1\n",
      " 0.09 0.1 ]\n",
      "Q_est = -6.31\n",
      "\n",
      "-> i_step = 39\n",
      "cur_state = [ 3 14]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12   -inf  -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 3.12\n",
      "Bset_idxes = [1, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -6.25\n",
      "weightedms_estimator\n",
      "idxes = [ 1  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.08 0.   0.1  0.11 0.11 0.   0.   0.1  0.09 0.1  0.   0.   0.1\n",
      " 0.11 0.09]\n",
      "Q_est = -6.10\n",
      "\n",
      "-> i_step = 40\n",
      "cur_state = [0 0]\n",
      "action = 2\n",
      "reward = 12.50\n",
      "new_state = [1 2]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72\n",
      " -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.54\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.08 0.08 0.1  0.09 0.09 0.   0.   0.08 0.1  0.1  0.   0.   0.09\n",
      " 0.1  0.08]\n",
      "Q_est = -4.45\n",
      "\n",
      "-> i_step = 41\n",
      "cur_state = [1 2]\n",
      "action = 7\n",
      "reward = -5.51\n",
      "new_state = [2 7]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72\n",
      " -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.54\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.08 0.11 0.07 0.08 0.1  0.   0.   0.09 0.08 0.08 0.   0.   0.1\n",
      " 0.1  0.11]\n",
      "Q_est = -4.78\n",
      "\n",
      "-> i_step = 42\n",
      "cur_state = [2 7]\n",
      "action = 1\n",
      "reward = 7.35\n",
      "new_state = [3 1]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72\n",
      " -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.54\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.09 0.1  0.09 0.08 0.08 0.   0.   0.1  0.1  0.09 0.   0.   0.09\n",
      " 0.1  0.09]\n",
      "Q_est = -4.69\n",
      "\n",
      "-> i_step = 43\n",
      "cur_state = [3 1]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37   -inf   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37 -15.38 -10.12  -1.81  -8.08  -9.72\n",
      " -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.54\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  8  9 10 13 14 15]\n",
      "probs = [0.   0.1  0.09 0.09 0.08 0.09 0.   0.   0.1  0.1  0.08 0.   0.   0.08\n",
      " 0.1  0.1 ]\n",
      "Q_est = -4.98\n",
      "\n",
      "-> i_step = 44\n",
      "cur_state = [0 0]\n",
      "action = 6\n",
      "reward = -7.88\n",
      "new_state = [1 6]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88 -15.38 -10.12  -1.81  -8.08\n",
      "  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.82\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  8  9 10 13 14 15]\n",
      "probs = [0.   0.09 0.09 0.08 0.08 0.07 0.09 0.   0.09 0.08 0.09 0.   0.   0.08\n",
      " 0.08 0.07]\n",
      "Q_est = -4.64\n",
      "\n",
      "-> i_step = 45\n",
      "cur_state = [1 6]\n",
      "action = 1\n",
      "reward = 17.05\n",
      "new_state = [2 1]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88 -15.38 -10.12  -1.81  -8.08\n",
      "  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.82\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  8  9 10 13 14 15]\n",
      "probs = [0.   0.09 0.09 0.08 0.09 0.08 0.09 0.   0.09 0.08 0.07 0.   0.   0.09\n",
      " 0.08 0.08]\n",
      "Q_est = -4.64\n",
      "\n",
      "-> i_step = 46\n",
      "cur_state = [2 1]\n",
      "action = 4\n",
      "reward = 5.61\n",
      "new_state = [3 4]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88 -15.38 -10.12  -1.81  -8.08\n",
      "  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.82\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  8  9 10 13 14 15]\n",
      "probs = [0.   0.09 0.08 0.08 0.07 0.08 0.09 0.   0.08 0.09 0.08 0.   0.   0.07\n",
      " 0.09 0.09]\n",
      "Q_est = -5.07\n",
      "\n",
      "-> i_step = 47\n",
      "cur_state = [3 4]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88   -inf -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88 -15.38 -10.12  -1.81  -8.08\n",
      "  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.82\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  8  9 10 13 14 15]\n",
      "probs = [0.   0.07 0.09 0.08 0.07 0.08 0.08 0.   0.1  0.09 0.08 0.   0.   0.07\n",
      " 0.1  0.08]\n",
      "Q_est = -5.07\n",
      "\n",
      "-> i_step = 48\n",
      "cur_state = [0 0]\n",
      "action = 7\n",
      "reward = -5.21\n",
      "new_state = [1 7]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.07 0.08 0.08 0.08 0.08 0.06 0.09 0.08 0.09 0.08 0.   0.   0.07\n",
      " 0.09 0.06]\n",
      "Q_est = -4.43\n",
      "\n",
      "-> i_step = 49\n",
      "cur_state = [1 7]\n",
      "action = 6\n",
      "reward = 20.97\n",
      "new_state = [2 6]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.08 0.08 0.08 0.07 0.08 0.08 0.07 0.1  0.08 0.07 0.   0.   0.08\n",
      " 0.06 0.07]\n",
      "Q_est = -4.87\n",
      "\n",
      "-> i_step = 50\n",
      "cur_state = [2 6]\n",
      "action = 6\n",
      "reward = 5.57\n",
      "new_state = [3 6]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.07 0.08 0.09 0.07 0.08 0.09 0.07 0.07 0.07 0.09 0.   0.   0.08\n",
      " 0.08 0.07]\n",
      "Q_est = -4.71\n",
      "\n",
      "-> i_step = 51\n",
      "cur_state = [3 6]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10\n",
      " 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 2\n",
      "action_maxlcb_muhat = 12.50\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  3.12  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.85\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.07 0.06 0.07 0.09 0.09 0.06 0.09 0.08 0.08 0.08 0.   0.   0.1\n",
      " 0.07 0.07]\n",
      "Q_est = -4.89\n",
      "\n",
      "-> i_step = 52\n",
      "cur_state = [0 0]\n",
      "action = 1\n",
      "reward = 5.55\n",
      "new_state = [1 1]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.45\n",
      "weightedms_estimator\n",
      "idxes = [ 2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.   0.08 0.07 0.07 0.09 0.07 0.09 0.09 0.08 0.09 0.   0.   0.09\n",
      " 0.09 0.1 ]\n",
      "Q_est = -5.94\n",
      "\n",
      "-> i_step = 53\n",
      "cur_state = [1 1]\n",
      "action = 6\n",
      "reward = -17.47\n",
      "new_state = [2 6]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.45\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.   0.08 0.08 0.08 0.08 0.09 0.08 0.08 0.07 0.1  0.   0.   0.08\n",
      " 0.09 0.09]\n",
      "Q_est = -5.65\n",
      "\n",
      "-> i_step = 54\n",
      "cur_state = [2 6]\n",
      "action = 4\n",
      "reward = 1.75\n",
      "new_state = [3 4]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.45\n",
      "weightedms_estimator\n",
      "idxes = [ 2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.   0.07 0.08 0.09 0.08 0.09 0.08 0.08 0.09 0.11 0.   0.   0.09\n",
      " 0.08 0.07]\n",
      "Q_est = -5.38\n",
      "\n",
      "-> i_step = 55\n",
      "cur_state = [3 4]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -21.96]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72 -21.96]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -4.45\n",
      "weightedms_estimator\n",
      "idxes = [ 2  3  4  5  6  7  8  9 10 13 14 15]\n",
      "probs = [0.   0.   0.07 0.09 0.08 0.07 0.08 0.08 0.07 0.09 0.08 0.   0.   0.09\n",
      " 0.11 0.08]\n",
      "Q_est = -5.85\n",
      "\n",
      "-> i_step = 56\n",
      "cur_state = [0 0]\n",
      "action = 15\n",
      "reward = -8.92\n",
      "new_state = [ 1 15]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.10\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.   0.   0.08 0.1  0.09 0.11 0.09 0.08 0.11 0.09 0.1  0.   0.   0.08\n",
      " 0.08 0.  ]\n",
      "Q_est = -4.17\n",
      "\n",
      "-> i_step = 57\n",
      "cur_state = [ 1 15]\n",
      "action = 12\n",
      "reward = -2.22\n",
      "new_state = [ 2 12]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.10\n",
      "weightedms_estimator\n",
      "idxes = [ 2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.   0.   0.09 0.1  0.09 0.1  0.1  0.1  0.1  0.08 0.08 0.   0.   0.09\n",
      " 0.08 0.  ]\n",
      "Q_est = -4.12\n",
      "\n",
      "-> i_step = 58\n",
      "cur_state = [ 2 12]\n",
      "action = 9\n",
      "reward = -8.68\n",
      "new_state = [3 9]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.10\n",
      "weightedms_estimator\n",
      "idxes = [ 1  2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.   0.   0.09 0.07 0.1  0.09 0.08 0.09 0.09 0.09 0.09 0.   0.   0.1\n",
      " 0.1  0.  ]\n",
      "Q_est = -4.29\n",
      "\n",
      "-> i_step = 59\n",
      "cur_state = [3 9]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[  -inf   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [  1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12  -1.81\n",
      "  -8.08  -9.72]\n",
      "Bset_nvisits = [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -3.10\n",
      "weightedms_estimator\n",
      "idxes = [ 2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.   0.   0.08 0.1  0.1  0.09 0.1  0.09 0.1  0.07 0.11 0.   0.   0.09\n",
      " 0.08 0.  ]\n",
      "Q_est = -4.13\n",
      "\n",
      "-> i_step = 60\n",
      "cur_state = [0 0]\n",
      "action = 0\n",
      "reward = -1.32\n",
      "new_state = [1 0]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  1  2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.09 0.   0.1  0.07 0.09 0.07 0.09 0.1  0.06 0.1  0.08 0.   0.   0.07\n",
      " 0.09 0.  ]\n",
      "Q_est = -3.56\n",
      "\n",
      "-> i_step = 61\n",
      "cur_state = [1 0]\n",
      "action = 4\n",
      "reward = 2.91\n",
      "new_state = [2 4]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.07 0.   0.1  0.08 0.09 0.08 0.09 0.07 0.09 0.08 0.08 0.   0.   0.09\n",
      " 0.07 0.  ]\n",
      "Q_est = -3.58\n",
      "\n",
      "-> i_step = 62\n",
      "cur_state = [2 4]\n",
      "action = 5\n",
      "reward = -22.52\n",
      "new_state = [3 5]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.07 0.   0.09 0.07 0.09 0.07 0.1  0.08 0.08 0.1  0.07 0.   0.   0.09\n",
      " 0.08 0.  ]\n",
      "Q_est = -4.10\n",
      "\n",
      "-> i_step = 63\n",
      "cur_state = [3 5]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   -inf   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 13 14]\n",
      "probs = [0.08 0.   0.07 0.08 0.07 0.09 0.09 0.1  0.08 0.07 0.09 0.   0.   0.09\n",
      " 0.08 0.  ]\n",
      "Q_est = -3.83\n",
      "\n",
      "-> i_step = 64\n",
      "cur_state = [0 0]\n",
      "action = 11\n",
      "reward = 6.52\n",
      "new_state = [ 1 11]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.34\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 11 13 14]\n",
      "probs = [0.08 0.   0.07 0.07 0.07 0.07 0.08 0.07 0.07 0.08 0.1  0.09 0.   0.08\n",
      " 0.07 0.  ]\n",
      "Q_est = -2.98\n",
      "\n",
      "-> i_step = 65\n",
      "cur_state = [ 1 11]\n",
      "action = 9\n",
      "reward = -17.97\n",
      "new_state = [2 9]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.34\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 11 13 14]\n",
      "probs = [0.07 0.   0.07 0.07 0.08 0.07 0.08 0.08 0.07 0.08 0.09 0.08 0.   0.08\n",
      " 0.08 0.  ]\n",
      "Q_est = -3.11\n",
      "\n",
      "-> i_step = 66\n",
      "cur_state = [2 9]\n",
      "action = 13\n",
      "reward = -19.33\n",
      "new_state = [ 3 13]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.34\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 11 13 14]\n",
      "probs = [0.07 0.   0.08 0.07 0.09 0.09 0.08 0.09 0.07 0.06 0.07 0.09 0.   0.08\n",
      " 0.07 0.  ]\n",
      "Q_est = -2.64\n",
      "\n",
      "-> i_step = 67\n",
      "cur_state = [ 3 13]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52   -inf  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.34\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 11 13 14]\n",
      "probs = [0.07 0.   0.08 0.08 0.08 0.07 0.09 0.06 0.09 0.08 0.07 0.07 0.   0.08\n",
      " 0.08 0.  ]\n",
      "Q_est = -3.21\n",
      "\n",
      "-> i_step = 68\n",
      "cur_state = [0 0]\n",
      "action = 12\n",
      "reward = -12.36\n",
      "new_state = [ 1 12]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "probs = [0.08 0.   0.07 0.06 0.08 0.07 0.06 0.09 0.07 0.07 0.06 0.07 0.07 0.09\n",
      " 0.07 0.  ]\n",
      "Q_est = -3.67\n",
      "\n",
      "-> i_step = 69\n",
      "cur_state = [ 1 12]\n",
      "action = 11\n",
      "reward = -8.86\n",
      "new_state = [ 2 11]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "probs = [0.07 0.   0.06 0.08 0.08 0.06 0.07 0.06 0.06 0.07 0.08 0.08 0.06 0.07\n",
      " 0.08 0.  ]\n",
      "Q_est = -3.61\n",
      "\n",
      "-> i_step = 70\n",
      "cur_state = [ 2 11]\n",
      "action = 9\n",
      "reward = -13.49\n",
      "new_state = [3 9]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "probs = [0.07 0.   0.07 0.09 0.08 0.07 0.06 0.07 0.07 0.05 0.07 0.08 0.07 0.08\n",
      " 0.07 0.  ]\n",
      "Q_est = -3.37\n",
      "\n",
      "-> i_step = 71\n",
      "cur_state = [3 9]\n",
      "action = 0\n",
      "reward = -10.00\n",
      "new_state = [0 0]\n",
      "terminated\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95  12.5   -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.97\n",
      "weightedms_estimator\n",
      "idxes = [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "probs = [0.06 0.   0.06 0.07 0.08 0.08 0.07 0.07 0.08 0.07 0.06 0.07 0.08 0.07\n",
      " 0.07 0.  ]\n",
      "Q_est = -4.02\n",
      "\n",
      "-> i_step = 72\n",
      "cur_state = [0 0]\n",
      "action = 2\n",
      "reward = 2.07\n",
      "new_state = [1 2]\n",
      "[1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[ -1.32   1.95   4.67  -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 7.83e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95   4.67  -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.98\n",
      "weightedms_estimator\n",
      "idxes = [ 0  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "probs = [0.07 0.   0.   0.1  0.09 0.07 0.08 0.09 0.07 0.07 0.08 0.08 0.08 0.08\n",
      " 0.07 0.  ]\n",
      "Q_est = -4.71\n",
      "\n",
      "-> i_step = 73\n",
      "cur_state = [1 2]\n",
      "action = 5\n",
      "reward = 2.23\n",
      "new_state = [2 5]\n",
      "[1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "[ -1.32   1.95   4.67  -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72 -20.8 ]\n",
      "[1.00e+10 1.17e+00 7.83e+00 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10\n",
      " 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.00e+10 1.16e+00]\n",
      "haver3_estimator\n",
      "action_maxlcb_idx = 1\n",
      "action_maxlcb_muhat = 1.95\n",
      "Bset_idxes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Bset_muhats = [ -1.32   1.95   4.67  -0.27  -0.62   2.37  -7.88  -5.21 -15.38 -10.12\n",
      "  -1.81   6.52 -12.36  -8.08  -9.72]\n",
      "Bset_nvisits = [1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Q_est = -2.98\n",
      "weightedms_estimator\n",
      "idxes = [ 0  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "probs = [0.08 0.   0.   0.07 0.09 0.07 0.07 0.08 0.07 0.07 0.08 0.07 0.09 0.07\n",
      " 0.09 0.  ]\n",
      "Q_est = -5.11\n",
      "\n",
      "-> i_step = 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "tdqm_disable = True\n",
    "\n",
    "# params\n",
    "num_trials = 1\n",
    "num_steps_train = 100\n",
    "num_episodes_eval = 100\n",
    "\n",
    "lr_sched_type = \"linear\"\n",
    "lr_sched_fn = create_lr_sched_fn(lr_sched_type, lr=0.7)\n",
    "\n",
    "max_eps = 1.0\n",
    "min_eps = 1.0\n",
    "decay_rate = 0.0001\n",
    "eps_sched_type = \"poly\"\n",
    "eps_sched_fn = create_eps_sched_fn(eps_sched_type, min_eps, max_eps, decay_rate)\n",
    "\n",
    "# create gym env\n",
    "env_id = \"gym_examples/NNWorldEnv01-v1\"\n",
    "env_scheme = \"two_island\"\n",
    "gamma = 0.95\n",
    "\n",
    "num_depths = 4\n",
    "num_widths = 16\n",
    "num_actions = num_widths\n",
    "terminal_reward = -10.0\n",
    "\n",
    "reward_dist = \"normal\"\n",
    "problem_instance = \"multi_gap_nonlinear\"\n",
    "action_max_mu = 0.0\n",
    "action_sigma = 10.0\n",
    "action_sigmas = action_sigma*np.ones(num_actions)\n",
    "gap_splits = [0.5]\n",
    "gap_deltas = [5.0]\n",
    "bandit_problem = BanditProblem(\n",
    "    problem_instance, reward_dist, num_actions, action_max_mu, \n",
    "    action_sigmas=action_sigmas, gap_splits=gap_splits, gap_deltas=gap_deltas)\n",
    "print(f\"action_mus = {bandit_problem.action_mus}\")\n",
    "print(f\"action_sigmas = {bandit_problem.action_sigmas}\")\n",
    "\n",
    "action_max_mu = bandit_problem.action_mus[0]\n",
    "optimal_num_steps = num_depths\n",
    "optimal_vstar = terminal_reward*gamma**(optimal_num_steps-1) \\\n",
    "    + action_max_mu*np.sum([gamma**k for k in range(optimal_num_steps-1)])\n",
    "optimal_reward_per_step = (terminal_reward + action_max_mu*(optimal_num_steps-1))/optimal_num_steps  \n",
    "print(f\"optimal_num_steps = {optimal_num_steps}\")\n",
    "print(f\"optimal_reward_per_step = {optimal_reward_per_step}\")\n",
    "print(f\"optimal_vstar = {optimal_vstar}\")\n",
    "\n",
    "env = gym.make(env_id, num_depths=num_depths, num_widths=num_widths, \n",
    "               bandit_problem=bandit_problem, terminal_reward=terminal_reward)\n",
    "env_wrapped = FlattenObservation(env)\n",
    "cur_state, info = env_wrapped.reset()\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "episode_start_sigmahats_list = manager.list()\n",
    "episode_rewards_list = manager.list()\n",
    "episode_vstar_est_list = manager.list()\n",
    "Q_table_list = manager.list()\n",
    "Q_nvisits_list = manager.list()    \n",
    "\n",
    "def run_trial(i_trial, args):\n",
    "\n",
    "    random.seed(10000+i_trial)\n",
    "    np.random.seed(10000+i_trial)\n",
    "\n",
    "    # env = gym.make(env_id, size=gridworld_size)\n",
    "    # env_wrapped = FlattenObservation(env)\n",
    "    # env_wrapped.reset(seed=10000+i_trial)\n",
    "\n",
    "    # lr_sched_fn = create_lr_sched_fn(lr_sched_type)\n",
    "    # eps_sched_fn = create_eps_sched_fn(eps_sched_type, min_eps, max_eps, decay_rate)\n",
    "    q_algo = create_q_algo(args[\"est_name\"])\n",
    "\n",
    "    Q_table, Q_nvisits, stats = q_algo(\n",
    "        env_wrapped, num_actions, num_steps_train,\n",
    "        gamma, lr_sched_fn, eps_sched_fn, tdqm_disable, args)\n",
    "\n",
    "    episode_start_sigmahats, episode_rewards, episode_vstar_est= zip(*stats)\n",
    "    episode_start_sigmahats_list.append(episode_start_sigmahats)\n",
    "    episode_rewards_list.append(episode_rewards)\n",
    "    episode_vstar_est_list.append(episode_vstar_est)\n",
    "    Q_table_list.append(Q_table)\n",
    "    Q_nvisits_list.append(Q_nvisits)\n",
    "\n",
    "args = dict()\n",
    "args[\"action_sigma\"] = action_sigma\n",
    "args[\"haver_alpha\"] = 2.0\n",
    "args[\"haver_delta\"] = 0.05\n",
    "args[\"haver_const\"] = 1.0\n",
    "args[\"weightedms_num_data\"] = 1000\n",
    "args[\"num_depths\"] = num_depths\n",
    "args[\"env_scheme\"] = env_scheme\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "episode_start_sigmahats_dict = defaultdict()\n",
    "episode_rewards_dict = defaultdict()\n",
    "episode_vstar_est_dict = defaultdict()\n",
    "episode_vstar_est_bias_dict = defaultdict()\n",
    "episode_vstar_est_var_dict = defaultdict()\n",
    "episode_vstar_est_mse_dict = defaultdict()\n",
    "Q_table_dict = defaultdict()\n",
    "Q_nvisits_dict = defaultdict()\n",
    "\n",
    "haver_const_ary = [1.0]\n",
    "haver_name_ary = [f\"haver_{x}\" for x in haver_const_ary]\n",
    "haver3_name_ary = [f\"haver3_{x}\" for x in haver_const_ary]\n",
    "\n",
    "est_name_ary = [\"max\", \"weightedms\"]\n",
    "# est_name_ary = haver_name_ary + est_name_ary \n",
    "# est_name_ary = est_name_ary + haver_name_ary\n",
    "# est_name_ary = est_name_ary + haver2_name_ary\n",
    "est_name_ary = est_name_ary + haver3_name_ary\n",
    "# est_name_ary = est_name_ary + haver4_name_ary\n",
    "est_name_ary = [\"haver3_1.0\"]\n",
    "for est_name in est_name_ary:\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n-> est_name = {est_name}\")\n",
    "    if \"haver\" in est_name:\n",
    "        elems = est_name.split(\"_\")\n",
    "        args[\"est_name\"] = elems[0]\n",
    "        args[\"haver_const\"] = float(elems[-1])\n",
    "        print(f\"haver_const = {args['haver_const']}\")\n",
    "    else:\n",
    "        args[\"est_name\"] = est_name\n",
    "    \n",
    "    pool.starmap(run_trial, [(i, args) for i in range(num_trials)])\n",
    "\n",
    "    episode_start_sigmahats_ary = np.hstack([episode_start_sigmahats_list])\n",
    "    episode_rewards_ary = np.hstack([episode_rewards_list])\n",
    "    episode_vstar_est_ary = np.hstack([episode_vstar_est_list])\n",
    "\n",
    "    episode_start_sigmahats_dict[est_name] = np.mean(episode_start_sigmahats_ary, 0)\n",
    "    episode_rewards_dict[est_name] = np.mean(episode_rewards_ary, 0)\n",
    "    episode_vstar_est_dict[est_name] = np.mean(episode_vstar_est_ary, 0)\n",
    "    print(f\"last_episode_start_sigmahat = {episode_start_sigmahats_dict[est_name][-1]:.4f}\")\n",
    "    print(f\"last_episode_reward_per_step = {episode_rewards_dict[est_name][-1]:.4f}\")\n",
    "    print(f\"last_episode_estim_start_muhat = {episode_vstar_est_dict[est_name][-1]:.4f}\")\n",
    "    \n",
    "    episode_vstar_est_bias_dict[est_name] = np.mean(episode_vstar_est_ary - optimal_vstar, 0)\n",
    "    episode_vstar_est_var_dict[est_name] = np.var(episode_vstar_est_ary - optimal_vstar, 0, ddof=1)\n",
    "    episode_vstar_est_mse_dict[est_name] = \\\n",
    "        episode_vstar_est_bias_dict[est_name]**2 \\\n",
    "        + episode_vstar_est_var_dict[est_name]\n",
    "    \n",
    "    # Q_table_dict[est_name] = np.mean(np.stack(Q_table_list), 0)\n",
    "    # Q_nvisits_dict[est_name] = np.mean(np.stack(Q_nvisits_list), 0)\n",
    "    # print(np.stack(Q_table_list).shape)\n",
    "    Q_table_dict[est_name] = np.stack(Q_table_list)[0,:,:,:]\n",
    "    Q_nvisits_dict[est_name] = np.stack(Q_nvisits_list)[0,:,:,:]\n",
    "    # print(Q_table_list[0][0,0,:])\n",
    "    # print(Q_table_list[1][0,0,:])\n",
    "    # print(Q_table_ary)\n",
    "    # stop\n",
    "                           \n",
    "    episode_start_sigmahats_list[:] = []\n",
    "    episode_rewards_list[:] = []\n",
    "    episode_vstar_est_list[:] = []\n",
    "    Q_table_list[:] = []\n",
    "    Q_nvisits_list[:] = []\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"it takes {end_time-start_time:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfeb72d-1e21-4c0f-bfd6-66ab5dac4bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4000 is out of bounds for axis 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1925033/1496431948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mest_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mest_name_ary_wanted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# axes[0].plot(x_ary, episode_rewards_dict[est_name][x_ary], label=est_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_ary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mest_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_ary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     axes[0].plot(\n\u001b[1;32m     14\u001b[0m         x_ary, y_ary, label=est_name)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4000 is out of bounds for axis 0 with size 100"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x1200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, axes = fig, axes = plt.subplots(\n",
    "        nrows=5, ncols=1, sharex=True, sharey=False, figsize=(8,12))\n",
    "# axes = [axes]\n",
    "\n",
    "x_ary = np.linspace(4000, num_steps_train-1, num=100, dtype=np.int32)\n",
    "# est_name_ary = [\"weightedms\"]\n",
    "est_name_ary_unwanted = []\n",
    "est_name_ary_wanted = [est_name for est_name in est_name_ary[:8] \\\n",
    "                 if est_name not in est_name_ary_unwanted]\n",
    "for est_name in est_name_ary_wanted:\n",
    "    # axes[0].plot(x_ary, episode_rewards_dict[est_name][x_ary], label=est_name)\n",
    "    y_ary = running_avg(episode_rewards_dict[est_name][x_ary], 100)\n",
    "    axes[0].plot(\n",
    "        x_ary, y_ary, label=est_name)\n",
    "    axes[1].plot(x_ary, episode_vstar_est_dict[est_name][x_ary], label=est_name)\n",
    "    axes[2].plot(x_ary, episode_vstar_est_mse_dict[est_name][x_ary], label=est_name)\n",
    "    axes[3].plot(x_ary, episode_vstar_est_bias_dict[est_name][x_ary], label=est_name)\n",
    "    axes[4].plot(x_ary, episode_vstar_est_var_dict[est_name][x_ary], label=est_name)\n",
    "\n",
    "axes[0].axhline(y=optimal_reward_per_step, color=\"black\")\n",
    "axes[1].axhline(y=optimal_vstar, color=\"black\")\n",
    "axes[0].set_title(f\"reward_per_step, K={num_actions}, num_depths={num_depths}, sigma={action_sigma},\"\n",
    "                    f\" delta={gap_deltas[0]}\")\n",
    "axes[1].set_title(\"vstar_est\")\n",
    "axes[2].set_title(\"vstar_est_mse\")\n",
    "axes[3].set_title(\"vstar_est_bias\")\n",
    "axes[4].set_title(\"vstar_est_var\")\n",
    "# axes[0].legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "# axes[2].legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b601f-0e05-4e14-bf6e-b8b8d348a72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = fig, axes = plt.subplots(\n",
    "        nrows=5, ncols=1, sharex=True, sharey=False, figsize=(8,12))\n",
    "# axes = [axes]\n",
    "\n",
    "x_ary = np.linspace(4000, num_steps_train-1, num=100, dtype=np.int32)\n",
    "# est_name_ary = [\"weightedms\"]\n",
    "est_name_ary_unwanted = [\"weightedms\", \"weightedms2\"]\n",
    "est_name_ary_wanted = [est_name for est_name in est_name_ary[:8] \\\n",
    "                 if est_name not in est_name_ary_unwanted]\n",
    "for est_name in est_name_ary_wanted:\n",
    "    # axes[0].plot(x_ary, episode_rewards_dict[est_name][x_ary], label=est_name)\n",
    "    y_ary = running_avg(episode_rewards_dict[est_name][x_ary], 100)\n",
    "    axes[0].plot(\n",
    "        x_ary, y_ary, label=est_name)\n",
    "    axes[1].plot(x_ary, episode_vstar_est_dict[est_name][x_ary], label=est_name)\n",
    "    axes[2].plot(x_ary, episode_vstar_est_mse_dict[est_name][x_ary], label=est_name)\n",
    "    axes[3].plot(x_ary, episode_vstar_est_bias_dict[est_name][x_ary], label=est_name)\n",
    "    axes[4].plot(x_ary, episode_vstar_est_var_dict[est_name][x_ary], label=est_name)\n",
    "\n",
    "axes[0].axhline(y=optimal_reward_per_step, color=\"black\")\n",
    "axes[1].axhline(y=optimal_vstar, color=\"black\")\n",
    "axes[0].set_title(f\"reward_per_step, K={num_actions}, num_depths={num_depths}, sigma={action_sigma},\"\n",
    "                    f\" delta={gap_deltas[0]}\")\n",
    "axes[1].set_title(\"vstar_est\")\n",
    "axes[2].set_title(\"vstar_est_mse\")\n",
    "axes[3].set_title(\"vstar_est_bias\")\n",
    "axes[4].set_title(\"vstar_est_var\")\n",
    "# axes[0].legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "# axes[2].legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3344b-55df-4701-b41a-01ef27b88cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for est_name in est_name_ary:\n",
    "    print(f\"\\n-> est_name = {est_name}\")\n",
    "    Q_table = Q_table_dict[est_name]\n",
    "    Q_nvisits = Q_nvisits_dict[est_name]\n",
    "    for i_row in range(num_depths):\n",
    "        for j_col in range(num_actions):\n",
    "            print(f\"{i_row} {j_col}: {Q_table[i_row,j_col]}, \\n     {Q_nvisits[i_row,j_col]}\")\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
